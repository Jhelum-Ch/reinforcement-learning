{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import itertools \n",
    "import gym\n",
    "from libr.envs.cliff_walking2 import CliffWalkingEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = CliffWalkingEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def offPolicyMCJC(env, numEpisodes, discount, epsilon, lenEp):\n",
    "    '''This is the main function for computing off-policy (epsilon-soft policy) Monte Carlo method.\n",
    "    Inputs - (i) env: OpenAI gym environment\n",
    "            (ii) numEpisodes: number of episodes\n",
    "            (ii) discount: the discount factor for infinite horizon discounted DP\n",
    "            (iv) epsilon: \n",
    "            (v) lenEp:\n",
    "    Outputs - Q-value and estimation policy  \n",
    "    For example, see Figure 5.7 of Sutton and Barto for the pseudocode\n",
    "    '''\n",
    "    \n",
    "    #Initialize\n",
    "    Q = np.zeros([env.observation_space.n,env.action_space.n])\n",
    "    N = np.zeros_like(Q)\n",
    "    D = np.zeros_like(Q)\n",
    "    aEstPolicy = np.zeros([env.observation_space.n,1])\n",
    "    \n",
    "    def estPolicy(Q, state):\n",
    "        estPol = np.argmax(Q[state])\n",
    "        return estPol\n",
    "    def behaviorSoftPolicy(Q, state, epsilon): #nA\n",
    "        if np.random.rand(1) > epsilon:\n",
    "            a = np.argmax(Q[state])\n",
    "        else:\n",
    "            #a = np.random.choice(nA,1)\n",
    "            a=env.action_space.sample()\n",
    "        return a\n",
    "    \n",
    "    for ep in range(numEpisodes):\n",
    "        episode = [] # start with an empty episode\n",
    "        currState = env.reset()\n",
    "        notMatchInstVec = [0]\n",
    "        rewardEp = 0 #initialize total discounted return from the episode\n",
    "        scale = 1\n",
    "        for t in range(lenEp):\n",
    "            #currAction = behaviorSoftPolicy(Q, currState, epsilon, env.action_space.n)\n",
    "            currAction = behaviorSoftPolicy(Q, currState, epsilon)\n",
    "            #generate a sample which returns nextstate, reward and if the episode terminates (done) \n",
    "            nextState, reward, done, _ = env.step(currAction)\n",
    "            #add the sample, which is a tuple (state,action,reward), to the episode\n",
    "            episode.append((currState, currAction, reward))\n",
    "            if done:\n",
    "                break\n",
    "            currState = nextState\n",
    "            rewardEp += discount*scale*reward #total discounted return from the episode\n",
    "            scale *= discount\n",
    "            \n",
    "            #lenEpisode = sum(1 for x in episode) # get the length of episode <= lenEp\n",
    "        saOccurence = set([(x[0], x[1]) for x in episode])\n",
    "        \n",
    "        for state, action in saOccurence:\n",
    "            aEstPolicyState = estPolicy(Q, state)\n",
    "            aEstPolicy[state] = aEstPolicyState  #estimation policy vector\n",
    "            if aEstPolicyState != action:\n",
    "                notMatchInstVec.append(t)\n",
    "                tau = t\n",
    "            else:\n",
    "                tau = notMatchInstVec[-1]\n",
    "            print(\"tau\", tau)\n",
    "            print(\"lenEpi\", len(episode))\n",
    "            #saTuple = (state,action)\n",
    "            #first occurence of (state,action) in episode after tau\n",
    "            episodeTauOnwards = episode[tau:]\n",
    "            print(\"episodeTau\",episodeTauOnwards)\n",
    "            first_occurence_idx = next(i for i,x in enumerate(episodeTauOnwards)\n",
    "                                       if x[0] == state and x[1] == action)\n",
    "\n",
    "            W = 1.0 #start backwards with the action of last instant in the episode\n",
    "            for k in range(len(episode)-1,first_occurence_idx,-1):\n",
    "                W *= 1/(episode[k][1])\n",
    "            N[state,action] += W*rewardEp\n",
    "            D[state,action] += W\n",
    "            Q[state,action] = N[state,action]/D[state,action]\n",
    "\n",
    "    return Q, aEstPolicy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tau 99\n",
      "lenEpi 100\n",
      "episodeTau [(0, 0, -1.0)]\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-c376f37d4a95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moffPolicyMCJC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-bf6a8b83c61e>\u001b[0m in \u001b[0;36moffPolicyMCJC\u001b[0;34m(env, numEpisodes, discount, epsilon, lenEp)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mepisodeTauOnwards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepisode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtau\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"episodeTau\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepisodeTauOnwards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             first_occurence_idx = next(i for i,x in enumerate(episodeTauOnwards)\n\u001b[0m\u001b[1;32m     66\u001b[0m                                        if x[0] == state and x[1] == action)\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "offPolicyMCJC(env,500, 0.9, 0.1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'trpow'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
