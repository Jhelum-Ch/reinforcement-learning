{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import permutations, repeat, product\n",
    "import gym\n",
    "from libr.envs.cliff_walking2 import CliffWalkingEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "env = CliffWalkingEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def onPolicyMCJC(env, numEpisodes, discount, epsilon, lenEp):\n",
    "    '''This is the main function for computing on-policy (epsilon-soft policy) Monte Carlo method.\n",
    "    Inputs - (i) env: OpenAI gym environment\n",
    "            (ii) numEpisodes: number of episodes\n",
    "            (ii) discount: the discount factor for infinite horizon discounted DP\n",
    "            (iv) epsilon: small non-negative scalar for epsilon-soft policy\n",
    "            (v) lenEp: maximum allowed length of episodes\n",
    "    Outputs - Q-Value and policy  \n",
    "    For example, see Figure 5.6 of Sutton and Barto for the pseudocode\n",
    "    '''\n",
    "    #initialize\n",
    "    #Q = np.zeros([env.nS, env.nA])\n",
    "    Q=np.zeros([env.observation_space.n,env.action_space.n])\n",
    "    returnCountFromEpisodes = np.zeros_like(Q)\n",
    "    returnSumFromEpisodes = np.zeros_like(Q)\n",
    "    policy = np.zeros([env.observation_space.n,1])\n",
    "    sumRewardFromEpisode = 0\n",
    "    \n",
    "    def epsilonSoft(state, Q, epsilon, nA):\n",
    "        pol = np.zeros([env.action_space.n])\n",
    "        aStar = np.argmax(Q[state])\n",
    "        for a in range(nA): #considering A(s) = nA for all s\n",
    "            pol[a] = epsilon/nA\n",
    "        pol[aStar] += 1-epsilon\n",
    "        return pol\n",
    "\n",
    "    for ep in range(numEpisodes):\n",
    "        episode = [] # start with an empty episode\n",
    "        currState = env.reset()\n",
    "        for t in range(lenEp):\n",
    "            epPolicy = epsilonSoft(currState,Q,epsilon,env.action_space.n)\n",
    "        \n",
    "            #randomly choose an action with prob assigned by epPolicy \n",
    "            currAction = np.random.choice(range(env.action_space.n),1, p=epPolicy)[0] \n",
    "        \n",
    "            #generate a sample\n",
    "            nextState, reward, done, _ = env.step(currAction)\n",
    "        \n",
    "            #add the sample to the episode\n",
    "            episode.append([currState, currAction, reward])\n",
    "        \n",
    "            if done:\n",
    "                break\n",
    "            currState = nextState\n",
    "        \n",
    "        #lenEpisode = sum(1 for x in episode) # get the length of episode \n",
    "        #stateEp = np.zeros([lenEpisode])\n",
    "        #actionEp = np.zeros([lenEpisode])\n",
    "        stateEp = np.zeros([len(episode)])\n",
    "        actionEp = np.zeros([len(episode)])\n",
    "        \n",
    "        for elem in range(len(episode)):\n",
    "            stateEp[elem] = episode[elem][0]\n",
    "            actionEp[elem] = episode[elem][1]\n",
    "        #li = [list(zip(stateEp, p)) for p in permutations(actionEp)] #list of lists\n",
    "        li = product(range(env.nS), range(env.nA))\n",
    "        flat_list = [item for sublist in li for item in sublist] #flatten the list of lists into one list\n",
    "        sa_list_of_tuples = set(flat_list) #retain unique elements of the list\n",
    "        sa_list_of_lists = [list(elem) for elem in sa_list_of_tuples]\n",
    "        \n",
    "        for state,action in sa_list_of_lists:\n",
    "            first_occ_step = next(idx for idx,x in enumerate(episode) if x[0] == state and x[1] == action)\n",
    "            \n",
    "            #return following first occurence of state, action\n",
    "            first_occ_return = sum((discount**t)*x[2] for t,x in enumerate(episode[first_occ_step:]))\n",
    "            \n",
    "            sumRewardFromEpisode += first_occ_return\n",
    "            returnSumFromEpisodes[state][action] = sumRewardFromEpisode #total return of (s,a) from all episodes\n",
    "            returnCountFromEpisodes[state][action] += 1 #total number of occurences of (s,a) in all episodes\n",
    "            Q[state,action] = returnSumFromEpisodes/returnCountFromEpisodes #average\n",
    "    policy = np.argmax(Q,1)        \n",
    "    return Q, policy\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "onPolicyMCJC(env, 500, 0.9, 0.1, 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
