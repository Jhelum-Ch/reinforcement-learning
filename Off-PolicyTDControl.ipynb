{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "import random\n",
    "import gym\n",
    "from libr.envs.cliff_walking2 import CliffWalkingEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = CliffWalkingEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def qLearningJC(env, numEpisodes, discount, epsilon):\n",
    "    '''This is the main function for computing off-policy TD0 method.\n",
    "    Inputs - (i) env: OpenAI gym environment\n",
    "            (ii) numEpisodes: number of episodes\n",
    "            (iii) discount: the discount factor for infinite horizon discounted DP\n",
    "            (iv) epsilon: small non-negative scalar for epsilon-greedy policy\n",
    "    Outputs - Q-Value, greedy policy\n",
    "    For example, see Figure 6.12 of Sutton and Barto for the pseudocode\n",
    "    '''\n",
    "    \n",
    "    #initialize\n",
    "    Q = np.zeros([env.observation_space.n,env.action_space.n])\n",
    "    alpha = 0.1 #learning rate\n",
    "    rList=[]\n",
    "    greedyPolicy=np.ones([env.observation_space.n,1]) #Greedy policy for all states\n",
    "    \n",
    "    def epsilonGreedyPolicy(state,Q,epsilon):\n",
    "        if np.random.rand(1) > epsilon:\n",
    "            a = np.argmax(Q[state])\n",
    "        else:\n",
    "            a = env.action_space.sample()\n",
    "        return a\n",
    "    \n",
    "    for ep in range(numEpisodes):\n",
    "        currState = env.reset()\n",
    "        rTotalFromEpisode = 0\n",
    "        for _ in itertools.count():\n",
    "            currAction = epsilonGreedyPolicy(currState,Q,epsilon)\n",
    "            \n",
    "            #generate a sample\n",
    "            nextState, reward, done, _ = env.step(currAction)\n",
    "            rTotalFromEpisode += reward\n",
    "            \n",
    "            TDerror = reward + discount*np.max(Q[nextState]) - Q[currState,currAction]\n",
    "            Q[currState,currAction] += alpha*TDerror\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            currState = nextState\n",
    "        rList.append(rTotalFromEpisode)\n",
    "    greedyPolicy = np.max(Q,1)\n",
    "    \n",
    "    return Q, greedyPolicy\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ -6.78084729,  -6.79271205,  -6.847377  ,  -6.80140967],\n",
       "        [ -6.64112771,  -6.63280667,  -6.65629485,  -6.62592065],\n",
       "        [ -6.41692215,  -6.4062033 ,  -6.43778504,  -6.3949237 ],\n",
       "        [ -6.16774649,  -6.14710661,  -6.14973546,  -6.14389942],\n",
       "        [ -5.86744489,  -5.83826708,  -5.85272877,  -5.86460827],\n",
       "        [ -5.50353125,  -5.49876119,  -5.51801434,  -5.53084717],\n",
       "        [ -5.17732488,  -5.11387806,  -5.12162024,  -5.17894744],\n",
       "        [ -4.68367885,  -4.67948671,  -4.71229246,  -4.75522376],\n",
       "        [ -4.20667678,  -4.2125883 ,  -4.24236576,  -4.34850579],\n",
       "        [ -3.74350846,  -3.71169308,  -3.72735318,  -3.7731774 ],\n",
       "        [ -3.16707181,  -3.20551517,  -3.18686672,  -3.23550135],\n",
       "        [ -2.65607002,  -2.66406101,  -2.65343296,  -2.7077994 ],\n",
       "        [ -6.9532841 ,  -6.95203168,  -6.96013575,  -6.95440869],\n",
       "        [ -6.7342367 ,  -6.73267406,  -6.7699865 ,  -6.76244921],\n",
       "        [ -6.48172116,  -6.47986877,  -6.49111461,  -6.49121863],\n",
       "        [ -6.20122253,  -6.17614721,  -6.18405756,  -6.21227522],\n",
       "        [ -5.84998467,  -5.83182574,  -5.83372522,  -5.87440846],\n",
       "        [ -5.50131171,  -5.45988645,  -5.45457329,  -5.58948054],\n",
       "        [ -5.03481853,  -5.02928498,  -5.03868162,  -5.02523972],\n",
       "        [ -4.55276234,  -4.54334687,  -4.55117498,  -4.77044523],\n",
       "        [ -4.05742241,  -4.00552748,  -4.00464295,  -4.03287714],\n",
       "        [ -3.50253761,  -3.38949689,  -3.3873594 ,  -3.5853822 ],\n",
       "        [ -3.01819881,  -2.69422507,  -2.69394405,  -3.01015024],\n",
       "        [ -2.44054788,  -2.15263137,  -1.89975613,  -2.5824416 ],\n",
       "        [ -7.1709592 ,  -7.17005622,  -7.39470239,  -7.23050797],\n",
       "        [ -6.87120076,  -6.85891899, -83.3228183 ,  -7.08017958],\n",
       "        [ -6.60183164,  -6.51171419, -92.02335569,  -6.63224507],\n",
       "        [ -6.2506551 ,  -6.12518682, -92.82102012,  -6.45229975],\n",
       "        [ -5.77125538,  -5.6950998 , -84.99053647,  -6.18894081],\n",
       "        [ -5.54233318,  -5.21695953, -79.41088679,  -5.75320761],\n",
       "        [ -4.87175828,  -4.68557595, -84.99053647,  -5.19733121],\n",
       "        [ -4.4581881 ,  -4.09509758, -89.05810109,  -4.81808654],\n",
       "        [ -3.93529285,  -3.43899973, -68.61894039,  -4.41140938],\n",
       "        [ -3.05524071,  -2.70999999, -79.41088679,  -3.55243362],\n",
       "        [ -2.82548939,  -1.9       , -81.46979811,  -2.77897596],\n",
       "        [ -2.3189597 ,  -1.55034513,  -1.        ,  -2.31032432],\n",
       "        [ -7.44881248, -99.36373146,  -7.49067055,  -7.49219313],\n",
       "        [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "        [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "        [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "        [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "        [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "        [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "        [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "        [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "        [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "        [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "        [  0.        ,   0.        ,   0.        ,   0.        ]]),\n",
       " array([-6.78084729, -6.62592065, -6.3949237 , -6.14389942, -5.83826708,\n",
       "        -5.49876119, -5.11387806, -4.67948671, -4.20667678, -3.71169308,\n",
       "        -3.16707181, -2.65343296, -6.95203168, -6.73267406, -6.47986877,\n",
       "        -6.17614721, -5.83182574, -5.45457329, -5.02523972, -4.54334687,\n",
       "        -4.00464295, -3.3873594 , -2.69394405, -1.89975613, -7.17005622,\n",
       "        -6.85891899, -6.51171419, -6.12518682, -5.6950998 , -5.21695953,\n",
       "        -4.68557595, -4.09509758, -3.43899973, -2.70999999, -1.9       ,\n",
       "        -1.        , -7.44881248,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qLearningJC(env,500,0.9,0.19)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
